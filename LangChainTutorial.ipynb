{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LangChain basics\n",
    "\n",
    "You can use prompt templates to insert data into your prompt easily. Don't forget to create environment variables for your OpenAI or Azure OpenAI keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "#  https://learn.microsoft.com/en-us/azure/ai-services/openai/reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"joke\": \"Why don\\'t scientists trust atoms? Because they make up everything!\"\\n}', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using completions API\n",
    "llm = AzureChatOpenAI(deployment_name='gpt-35-turbo',temperature=0)\n",
    "\n",
    "# invoke llm\n",
    "llm(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content='Tell me a joke and output the result in json.'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LangChain to get structured data outputs\n",
    "\n",
    "You can use LangChain to get JSON responses. Below I am wanting my question and answer in a structured format. But you can build whatever models you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am chosing Q&A here, but really you can chose any structure you need for your class. \n",
    "# It knows how to generate the class structure based on what you put in description - magic?\n",
    "class QnA(BaseModel):\n",
    "    question: str = Field(description=\"question\")\n",
    "    answer: str = Field(description=\"answer\")\n",
    "    # word: str = Field(description=\"the first word of the question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use different parsers, or even construct your own. Pydantic creates nice objects for python so I am using that.\n",
    "parser = PydanticOutputParser(pydantic_object=QnA)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Answer the user query.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"description\": \"question\", \"type\": \"string\"}, \"answer\": {\"title\": \"Answer\", \"description\": \"answer\", \"type\": \"string\"}}, \"required\": [\"question\", \"answer\"]}\\n```\\nWhat is a good name for a company that makes colorful socks?\\n'\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt\n",
    "actor_query = \"What is a good name for a company that makes colorful socks?\"\n",
    "_input = prompt.format_prompt(query=actor_query)\n",
    "print(_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"question\": \"What is a good name for a company that makes colorful socks?\", \"answer\": \"Rainbow Threads\"}\n"
     ]
    }
   ],
   "source": [
    "# invoke the llm using the prompt template and formatter\n",
    "output = llm([HumanMessage(content=_input.to_string())])\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following demonstrates how to use Conversation History and Memory in a conversation chain\n",
    "\n",
    "LangChain can help remember what the user is saying for back and forth chatgpt like capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What does Microsoft do?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Microsoft is a multinational technology company that develops, manufactures, licenses, supports, and sells computer software, consumer electronics, personal computers, and related services. They are best known for their operating system, Microsoft Windows, which is used by the majority of personal computers worldwide. Microsoft also offers a wide range of other software products, including the Microsoft Office suite, which includes popular applications like Word, Excel, and PowerPoint. Additionally, they provide cloud services through their Azure platform, develop video game consoles like the Xbox, and have a presence in the hardware market with products like the Surface line of tablets and laptops.'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "# llm = AzureChatOpenAI(deployment_name='gpt-35-turbo',temperature=0)\n",
    "\n",
    "# The conversation object manages the back and forward conversation, including memory.\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "# We just add user input and let the conversation object handle the rest.\n",
    "conversation.predict(input=\"What does Microsoft do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What does Microsoft do?\n",
      "AI: Microsoft is a multinational technology company that develops, manufactures, licenses, supports, and sells computer software, consumer electronics, personal computers, and related services. They are best known for their operating system, Microsoft Windows, which is used by the majority of personal computers worldwide. Microsoft also offers a wide range of other software products, including the Microsoft Office suite, which includes popular applications like Word, Excel, and PowerPoint. Additionally, they provide cloud services through their Azure platform, develop video game consoles like the Xbox, and have a presence in the hardware market with products like the Surface line of tablets and laptops.\n",
      "Human: How can they partner together with NASA?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Microsoft has a long-standing partnership with NASA. They collaborate on various projects and initiatives that involve the use of technology in space exploration and research. One notable example is the use of Microsoft's HoloLens mixed reality headset in NASA's Project Sidekick, which aims to provide astronauts with virtual assistance during space missions. Microsoft also works with NASA on data analysis and visualization tools, as well as cloud computing solutions to support the agency's scientific research and data processing needs. Additionally, Microsoft has partnered with NASA to develop educational programs and initiatives to inspire and engage students in STEM fields.\""
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding more user input to the conversation chain keeps the memory, see the verbose output below.\n",
    "conversation.predict(input=\"How can they partner together with NASA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Tools\n",
    "\n",
    "You can set custom tools to be used, for example here is the bing search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"BING_SUBSCRIPTION_KEY\"] = \"\"\n",
    "# os.environ[\"BING_SEARCH_URL\"] = \"https://api.bing.microsoft.com/v7.0/search\"\n",
    "\n",
    "# from langchain.utilities import BingSearchAPIWrapper\n",
    "# search = BingSearchAPIWrapper(k=1)\n",
    "# search.run(\"Dave Enright from Singapore on LinkedIn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use agents to orchestrate multiple tools together\n",
    "\n",
    "Below the agent is being told to use the Bing Search tool when it needs to answer current events.The agent will use this tool based on the kinds of questions the user is asking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\n",
    "# from langchain.utilities import BingSearchAPIWrapper\n",
    "\n",
    "# # The tool description helps langchain work out when to use the tool. Be careful with your descriptions as its not completely deterministic.\n",
    "# search = BingSearchAPIWrapper(k=1)\n",
    "# tools = [\n",
    "#     Tool(\n",
    "#         name = \"Intermediate Answer\",\n",
    "#         func=search.run,\n",
    "#         description=\"Use this when you need up to date information that is timely\",\n",
    "#         return_direct=True\n",
    "#     )\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the agent to use the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Tuple, Any, Union\n",
    "# from langchain.agents import initialize_agent\n",
    "# from langchain.agents import AgentType\n",
    "\n",
    "# # There are different kinds of agents documented on the langchain website. This particular one is basically just a chat with search agent.\n",
    "# self_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\n",
    "# self_ask_with_search.run(\"Give me a list of to places to visit where weather is not too hot and do not have a lot of tourists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.agents import load_tools\n",
    "# from langchain.agents import initialize_agent\n",
    "# from langchain.agents import AgentType\n",
    "# from langchain.llms import OpenAI\n",
    "\n",
    "# llm = OpenAI(temperature=0)\n",
    "# tools = load_tools([\"bing-search\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "# agent.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# There are different document loaders available, this is the basic PDF loader.\n",
    "loader = PyPDFLoader(\"Benefit_Options.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Contoso Electronics  \\nPlan and Benefit Packages', metadata={'source': 'Benefit_Options.pdf', 'page': 0})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting into pages is useful to give page number back as a source\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: offers a wider range of prescription drug coverage than Northwind Standard. Both plans offer coverage \n",
      "for vision and dental services, as well as medical services.  \n",
      "Next Steps  \n",
      "We hope that this information has been helpful in understanding the differe nces between Northwind \n",
      "Health Plus and North\n",
      "2: Welcome to Contoso  Electronics ! We are excited to offer our employees two comprehensive health \n",
      "insurance plans through Northwind Health.  \n",
      "Northwind Health Plus  \n",
      "Northwind Health Plus is a comprehensive plan that provides comprehensive coverage for medical, \n",
      "vision, and dental services. T his pl\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# This is creating an index based on embeddings, but as you can see by the output it's a bit ugly to work with.\n",
    "\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings(chunk_size=1))\n",
    "docs_pdf = faiss_index.similarity_search(\"What's the difference between plus and standard?\", k=2)\n",
    "for doc in docs_pdf:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work yet with Azure.\n",
    "#from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "# Using the vector store from the loader means langchain can abstract away a lot of the under the hood stuff.\n",
    "# Note I am using a local python package for the vectorDB. You may get errors.\n",
    "#index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'The main difference between Northwind Health Plus and Northwind Standard is that Northwind Health Plus offers more comprehensive coverage. Northwind Health Plus provides coverage for emergency services, mental health and substance abuse, and out-of-network services, while Northwind Standard does not. Additionally, Northwind Health Plus offers a wider range of prescription drug coverage, including generic, brand-name, and specialty drugs, whereas Northwind Standard only covers generic and brand-name drugs. Both plans offer coverage for vision and dental services, as well as medical services. \\nSOURCES: Benefit_Options.pdf'}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "chain_pdf = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "query = \"What's the difference between plus and standard?\"\n",
    "chain_pdf({\"input_documents\": docs_pdf, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain CSV example\n",
    "\n",
    "from langchain.agents import create_csv_agent\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "agent = create_csv_agent(llm, 'sample.csv', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the number of rows in the dataframe, I can use the `shape` attribute of the dataframe.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: df.shape[0]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m5\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe dataframe has 5 rows.\n",
      "Final Answer: 5\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"how many rows are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text_splitter = CharacterTextSplitter()\n",
    "\n",
    "with open(\"state_of_the_union.txt\", encoding=\"utf8\") as f:\n",
    "    sotu = f.read()\n",
    "texts = text_splitter.split_text(sotu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "docs_txt = [Document(page_content=t) for t in texts[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'President Biden addresses the conflict between Russia and Ukraine in his State of the Union address, emphasizing the unity and resolve of the United States and its allies in holding Russia accountable. The US is taking action against Russian oligarchs and corrupt leaders, implementing economic sanctions, and providing support to Ukraine. American forces are mobilized to defend NATO allies, not engaged in the conflict directly. The US is implementing targeted sanctions on the Russian economy and releasing oil reserves to help with gas prices. The speaker highlights the rise of democracies and the support for Ukraine worldwide. The American Rescue Plan is praised for providing relief and support to Americans, and the success of the plan in creating jobs and stimulating economic growth is highlighted. The need to invest in infrastructure is emphasized, with gratitude for bipartisan support for the infrastructure law.'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "chain_summarize_txt = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "chain_summarize_txt.run(docs_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, the President did not say \"selamat pagi\" in English.'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain_qa_txt = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "query = \"Did the president say selamat pagi in english?\"\n",
    "chain_qa_txt.run(input_documents=docs_txt, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test using cog search vector store.\n",
    "# reference: https://python.langchain.com/docs/integrations/vectorstores/azuresearch\n",
    "# !pip install --index-url=https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/ azure-search-documents==11.4.0a20230509004\n",
    "# !pip install azure-identity\n",
    "# !pip install azure-search-documents==11.4.0b6\n",
    "# !pip install langchain --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.vectorstores.azuresearch import AzureSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_address: str = 'https://globecogse.search.windows.net'\n",
    "vector_store_password: str = 'Jk3aMWZNUeOdjiPZxDWgaWS4lXOY001YQcRTZXVUbZAzSeCiFO0l'\n",
    "index_name: str = 'globevectorindex'\n",
    "\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(chunk_size=1)  \n",
    "vector_store_union: AzureSearch = AzureSearch(azure_search_endpoint=vector_store_address,  \n",
    "                                        azure_search_key=vector_store_password,  \n",
    "                                        index_name=index_name,  \n",
    "                                        embedding_function=embeddings.embed_query) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "loader_cog_txt = TextLoader('state_of_the_union.txt', encoding='utf-8')\n",
    "documents_union_txt = loader_cog_txt.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1434, which is longer than the specified 1000\n",
      "Created a chunk of size 3063, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs_union_txt = text_splitter.split_documents(documents_union_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NmE5M2M2N2MtMjY2MC00NDllLTkyY2UtYjFjYjcxNjQ2ZjFj',\n",
       " 'YTYzMTVhZDgtMjkzYy00ODA5LTg4Y2QtNzQ1YTM2NmQyMjVm',\n",
       " 'ZGYzMTBlMGItYzk1ZS00ZDgwLWJlYjYtMDU0M2NhNDA3Y2Y4',\n",
       " 'NjEyNjE1MzUtZjVhNC00M2MxLWIzYzAtMzFjMjQ3OWQwZjhj',\n",
       " 'YWE0ZmM0MmEtNGJiYS00YzFjLTk4YjQtYmJiZDZkMGU3NGJl',\n",
       " 'MDc2YzIxNGEtZWY3Yi00MDRjLWI5OTctMmQ2MmViMTJjMGNj',\n",
       " 'NTE0OTdiY2EtNDcxNi00MjY0LWE3NTEtZWViYTc5OTBmMTA0',\n",
       " 'ZTgzNzU3NDktMjMzOS00ZjFiLTlkYWItMzU3ZjJjMjViZTFh',\n",
       " 'OWMwZWVhMDYtMGRjZC00ZDA1LWI0MDMtMTMzMDAyN2IwZWNh',\n",
       " 'YzhhZWE5ZGItMGE2MC00ZTc0LWJkYzMtNWMwNmRhOGMwOWVk',\n",
       " 'NzZhZjk4OGYtMjQ1MC00NDgyLTlkNzctNmM1OTc4ODI2Nzhj',\n",
       " 'OTViOWU0MWQtMTBhMS00Mzg5LTk0NjQtNTU2MTFhZjQxOTll',\n",
       " 'ZTdmY2RlNDAtMmZkNC00YWNmLTg1NDgtOGEwNjE2NWFjYmFi',\n",
       " 'OGI1ZjhmOWMtY2NhMC00M2ZkLTk2NjYtNDdmYTJiMzFjZDc2',\n",
       " 'ODQ1ZmU2OTctM2ZlYy00NjI1LTliYTUtMDhlZTUzZjNmZTNi',\n",
       " 'NmFiMzQ0MDQtMGU1Ni00Nzc1LWFhODUtZWNjOGJjMDM5ZDY4',\n",
       " 'NzE2MmE0MjYtZDc1Ny00ZDBmLWFhNGEtM2IwMGNmZWU3YmFj',\n",
       " 'NzkxMzQ2OGItZWEyNy00MzkxLTkyNGYtYzNjMDVkOWQxMzIz',\n",
       " 'ODM3ZThmZDgtYWVkYS00ZmMzLWJkYjktM2E2MWE1MDUyZmU2',\n",
       " 'NTU0NjU5MTEtM2RkYy00OGRhLThhZjEtMGQyNzk0M2E3OWUy',\n",
       " 'YTU2MDZkZjUtMzVjYS00OGY3LWFlOGUtYWZhZGJlNWYxYTQ4',\n",
       " 'ZjRhNzQ3MTgtODRkNi00NWY1LThjMjktNDg1YjI5MTZjZmZj',\n",
       " 'ZjRiNjk5MTEtNmVkZC00ZWQxLThjYTMtZjNhMTI5MDQ3YWFi',\n",
       " 'NjZjZWRmZTQtZTM5OC00MDMyLWJiODctODhjNjNhODVmOGY2',\n",
       " 'MWRkNzI1MDItYWQ4ZS00YzNhLThmYTEtZTA1YjI4ZDU2M2Vi',\n",
       " 'NzAxYjJlY2MtNzdkMC00OGIxLWI1NzgtZmExNjg5ZmM0ZWM5',\n",
       " 'MDE0NmMzYTctMjNkMy00NmJkLTg1MWEtNGFkYzg3ODNjNDYy',\n",
       " 'YmZkZTIxMzEtOTZmNS00MmM2LWJhOTQtOTZkM2MxMWFmMjAw',\n",
       " 'NTFiNzQ2ZDAtMTIyZS00N2YzLTkzMzEtOWU1ZjNmNDJmYzc1',\n",
       " 'M2JjNGFiODUtNTBiOC00ZGU2LWI5NTUtZjljMjhlNmNiZjcy',\n",
       " 'MGNlNjA0YWQtMmUwNy00YmU0LTg2NzItMGE4ZmQ3NzUzNmIz',\n",
       " 'NGY1NTEwNDktNWYzYi00NDFiLWI5NDYtNTk1YmU1OTQ4NzIw',\n",
       " 'ZWMzY2ZiNTgtMmQ5Yi00MGJkLThhMmMtMjZiMzMyZjg3MDIy',\n",
       " 'MDNlNDM4YTYtOWM5OS00YzI2LWE0YTUtZjk1MGMwYzQyNWFk',\n",
       " 'MzY4OGExOGYtMGJiOS00YzI3LWJkNzctNWE5YTdhN2ZmZWQy',\n",
       " 'OWQ4MzBiMzgtMTJjYS00NzYxLTljYTQtMzI1YmU4NmFlZjBk',\n",
       " 'NTBkZTk3ZGUtMzA3ZC00MWZjLTg2YTUtNDk0Yjc0YTUyOTFj',\n",
       " 'NDE3NmRjOTUtMTE3Mi00NTQ0LWE0MmItNDJhMTY0NTk3NzVm',\n",
       " 'ZGI3MmRlYmMtMWQyMS00ZmE5LWI1Y2YtNjUzYzcwNzEwOWYz',\n",
       " 'NmI5MDg4ZDctMmNhOC00NWQzLTlhZmUtMDkyNWQzYTg1MzY3',\n",
       " 'OTkxYzY5ZTktNzkwYS00NGJhLWI2ZjktZDhhMjQ5MWQ5MGNi',\n",
       " 'OTM5ZTMxZjYtMGM3Zi00NmNmLTg3ZWEtZTY5NmNjOTI4MDFi',\n",
       " 'NWMyMTdmOTEtMjZlYS00NWMyLTk0MGQtMzIxYTcwYTgyMDMy',\n",
       " 'YTEyZGY1M2YtMDIwNi00YWYxLWJjMmEtY2RiOGI4OGQzYTc0',\n",
       " 'N2Q3OTdhY2YtZjNiNi00MGJiLWI5MTctODE5Mzc4N2JiOWM4',\n",
       " 'MGFhZmQzNDctZmE2ZS00YTA5LWIyZDctMGM3YmIyOThhN2Zm',\n",
       " 'ZGM4NGM1NDUtZDQzNC00ODlhLWJhZWEtODJjNjM5ODMxYjU2',\n",
       " 'MjEwZTAwYzAtY2Y2MC00MTc2LTk1ZDAtYTg0ZjE2NzJlOTcx',\n",
       " 'NjkwM2E1ZmItZjE2MS00OGQyLWIyMDMtMjQ4YWI2ODZkY2Zj',\n",
       " 'OTg1OWQ4MjItNGI3MS00NmQxLTgzMTUtZTU4MmQyZTI0Y2Jj',\n",
       " 'OTZmZWY3OTEtMDIwNy00YWZjLTlkZGEtM2E4ZWNhOTI3OTUw',\n",
       " 'YjBhOTI1OGQtOTIxMi00YzZiLThiYzgtYzRkZmJiMGUwZTNk',\n",
       " 'NmQ3YmVhMWQtN2ViNS00OTJlLTg3NzgtMjJjMTU0NTU5NTgz',\n",
       " 'MzRkYjlmYzItYjMxYi00NmJkLWFhMzktYTAxODEwN2U2OWM3',\n",
       " 'NWI0NjgzNDgtYzA4YS00OWM1LTk0ZTAtMWVkNWJlOGNjZDAw',\n",
       " 'Yjg5ZWNhMTktYThjZi00Yzg0LWJmMDItMzRmNmJhZmI2Y2Nk',\n",
       " 'MzFlMDcyOGUtOTI5My00NmM1LWExOTUtMDhmMTgzZmJlMzgw',\n",
       " 'ZWVkN2M2NTMtOTFlMC00ZWFiLThhMmMtZjE2ZWE0NDUyOGJj',\n",
       " 'ZWMxOTg0ZDItMjM4Mi00NGYzLWI2MjktMDRhNWE3NzQwYjAz',\n",
       " 'ODJiZDFkNDQtZWJlNC00MmMyLThmNTAtMTcwZjNiYmZmYWJj',\n",
       " 'MzBhM2ZlNzAtZWRkMC00ZGE0LWIwOGEtZTU5OTY3Y2QxYjE2',\n",
       " 'ZTA5M2QwYzctNzNmOC00MmY2LTgyNTQtZTE1MzlkYzgwNGMx',\n",
       " 'MmM3MTUwN2YtYzdlYS00NTAyLThiZmUtM2NlNmVkOGU5Mzdh',\n",
       " 'NjQ0NTNhNGEtYTc5Yy00YmNhLWE4MTItM2NkNTJiYzk5NTY1']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_union.add_documents(documents=docs_union_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The president said that he has nominated Ketanji Brown Jackson to serve on the United States Supreme Court.'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using retrievalQA\n",
    "from langchain.chains import RetrievalQA\n",
    "qa_cog_union = RetrievalQA.from_llm(llm=llm, retriever=vector_store_union.as_retriever(), verbose=False)\n",
    "qa_cog_union.run('What did the president say about Ketanji Brown Jackson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And we all know—no matter what your ideology, we all know one of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. As I did 4 days ago, I've nominated a Circuit Court of Appeals—Ketanji Brown Jackson.\n"
     ]
    }
   ],
   "source": [
    "# using similarity search\n",
    "docus_similar_union = vector_store_union.similarity_search(query=\"What did the president say about Ketanji Brown Jackson\", k=3, search_type='similarity')\n",
    "print(docus_similar_union[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectoreStoreIndex (Not Working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use custom index with existing vectorstore. This opens up the ability to do query_with_sources etc\n",
    "# from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "# vector_store: AzureSearch = AzureSearch(azure_search_endpoint=vector_store_address,  \n",
    "#                                         azure_search_key=vector_store_password,  \n",
    "#                                         index_name=index_name,  \n",
    "#                                         embedding_function=embeddings.embed_query) \n",
    "\n",
    "# index_wrapper = VectorstoreIndexCreator(\n",
    "#     vectorstore_cls=AzureSearch,\n",
    "#     embedding=OpenAIEmbeddings(chunk_size=1),\n",
    "#     text_splitter=CharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "# index_wrapper.query_with_sources(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Drive Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GoogleDriveLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader_drive = GoogleDriveLoader(document_ids=[\"1s_-umZEhfcNVcyEZ6FbOi0pPIr-UEadEbz_OyAerpqc\"],credentials_path='cred-globe.json')\n",
    "# loader_drive = GoogleDriveLoader(document_ids=[\"15pprQ_7recgz1aKNkeITReDjGPVOywzm0jDqMKLHdJA\"])\n",
    "\n",
    "# docs_drive = loader_drive.load()\n",
    "# print(f'{len(docs_drive)} document/s, {len(docs_drive[0].page_content)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tracks offered by the cadetship program are AI/ML, Software Development, Product Management and Delivery, Security, IT Quality Management and Testing, and Platform Management.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document question and aswering\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "query = 'What are the tracks offered by the cadetship program?'\n",
    "chain_drive = load_qa_chain(llm, chain_type='stuff', verbose=False)\n",
    "chain_drive.run(input_documents=docs_drive,question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: 992 characters\n",
      "Document 2: 546 characters\n",
      "Document 3: 947 characters\n",
      "Document 4: 913 characters\n",
      "Document 5: 717 characters\n",
      "Document 6: 964 characters\n",
      "Document 7: 915 characters\n",
      "Document 8: 769 characters\n"
     ]
    }
   ],
   "source": [
    "# text splitting \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter_drive = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs_split_drive = text_splitter_drive.split_documents(docs_drive)\n",
    "\n",
    "for i,doc in enumerate(docs_split_drive):\n",
    "    print(f'Document {i+1}: {len(docs_split_drive[i].page_content)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZjdjMGFkNzAtN2QyYi00Nzg0LTlkYmQtODBhZDkzZTk5ZDk3',\n",
       " 'NWYzNWQwMWMtYWM3MC00MTAwLTk4ZTEtZTA1MzcwNTBjMzc1',\n",
       " 'OGIwMmFiNmQtY2YyNC00YTI5LTkxNjItNGJkMzBjYTVmMjgz',\n",
       " 'ODdiZGIwOTYtYjEwNS00YzhlLThjZmUtMDcyZWIwYzQ4MzQz',\n",
       " 'ZmIyYTQ1NjItZTdlYS00OTU3LTg4MTMtZjljZjIwZDZkODg2',\n",
       " 'NzAwMmU4NDEtZTEzNi00ZGEyLTgwZjktMWFiYTUwZDNhODZh',\n",
       " 'MzViZTdjM2EtNDY4NC00NzdiLTgyY2ItNzU5YThhNDQyODY1',\n",
       " 'ZTE2NTFjNDktZGEwYS00NTk0LTliZGYtN2EzMjM1N2JmNDY2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "vector_store_address: str = 'https://globecogse.search.windows.net'\n",
    "vector_store_password: str = 'Jk3aMWZNUeOdjiPZxDWgaWS4lXOY001YQcRTZXVUbZAzSeCiFO0l'\n",
    "index_name: str = 'globevectorindex'\n",
    "\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(chunk_size=1)  \n",
    "vector_store_drive: AzureSearch = AzureSearch(azure_search_endpoint=vector_store_address,  \n",
    "                                        azure_search_key=vector_store_password,  \n",
    "                                        index_name=index_name,  \n",
    "                                        embedding_function=embeddings.embed_query) \n",
    "                                        \n",
    "vector_store_drive.add_documents(documents=docs_split_drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RetrievalQA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# from langchain.chains import RetrievalQA\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m qa_drive \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_llm(llm\u001b[39m=\u001b[39mllm, retriever\u001b[39m=\u001b[39mvector_store_drive\u001b[39m.\u001b[39mas_retriever(k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m qa_drive\u001b[39m.\u001b[39mrun(\u001b[39m'\u001b[39m\u001b[39mHow do I apply for the technical development program?\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RetrievalQA' is not defined"
     ]
    }
   ],
   "source": [
    "# from langchain.chains import RetrievalQA\n",
    "qa_drive = RetrievalQA.from_llm(llm=llm, retriever=vector_store_drive.as_retriever(k=1), verbose=True)\n",
    "qa_drive.run('How do I apply for the technical development program?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docus_similar_drive = vector_store_drive.similarity_search(query=\"How do I apply for the technical development program?\", k=3, search_type='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_drive = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store_drive.as_retriever(k=2),verbose=True)\n",
    "qa_drive.run('How do I apply for the technical development program?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
