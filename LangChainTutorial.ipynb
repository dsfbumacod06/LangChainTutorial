{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LangChain basics\n",
    "\n",
    "You can use prompt templates to insert data into your prompt easily. Don't forget to create environment variables for your OpenAI or Azure OpenAI keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "#  https://learn.microsoft.com/en-us/azure/ai-services/openai/reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"joke\": \"Why don\\'t scientists trust atoms? Because they make up everything!\"\\n}', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using completions API\n",
    "llm = AzureChatOpenAI(deployment_name='gpt-35-turbo',temperature=0)\n",
    "\n",
    "# invoke llm\n",
    "llm(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content='Tell me a joke and output the result in json.'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LangChain to get structured data outputs\n",
    "\n",
    "You can use LangChain to get JSON responses. Below I am wanting my question and answer in a structured format. But you can build whatever models you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am chosing Q&A here, but really you can chose any structure you need for your class. \n",
    "# It knows how to generate the class structure based on what you put in description - magic?\n",
    "class QnA(BaseModel):\n",
    "    question: str = Field(description=\"question\")\n",
    "    answer: str = Field(description=\"answer\")\n",
    "    # word: str = Field(description=\"the first word of the question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use different parsers, or even construct your own. Pydantic creates nice objects for python so I am using that.\n",
    "parser = PydanticOutputParser(pydantic_object=QnA)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Answer the user query.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"description\": \"question\", \"type\": \"string\"}, \"answer\": {\"title\": \"Answer\", \"description\": \"answer\", \"type\": \"string\"}}, \"required\": [\"question\", \"answer\"]}\\n```\\nWhat is a good name for a company that makes colorful socks?\\n'\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt\n",
    "actor_query = \"What is a good name for a company that makes colorful socks?\"\n",
    "_input = prompt.format_prompt(query=actor_query)\n",
    "print(_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"question\": \"What is a good name for a company that makes colorful socks?\", \"answer\": \"Rainbow Threads\"}\n"
     ]
    }
   ],
   "source": [
    "# invoke the llm using the prompt template and formatter\n",
    "output = llm([HumanMessage(content=_input.to_string())])\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following demonstrates how to use Conversation History and Memory in a conversation chain\n",
    "\n",
    "LangChain can help remember what the user is saying for back and forth chatgpt like capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What does Microsoft do?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Microsoft is a multinational technology company that develops, manufactures, licenses, supports, and sells computer software, consumer electronics, personal computers, and related services. They are best known for their operating system, Microsoft Windows, which is used by the majority of personal computers worldwide. Microsoft also offers a wide range of other software products, including the Microsoft Office suite, which includes popular applications like Word, Excel, and PowerPoint. Additionally, they provide cloud services through their Azure platform, develop video game consoles like the Xbox, and have a presence in the hardware market with products like the Surface line of tablets and laptops.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "# llm = AzureChatOpenAI(deployment_name='gpt-35-turbo',temperature=0)\n",
    "\n",
    "# The conversation object manages the back and forward conversation, including memory.\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "# We just add user input and let the conversation object handle the rest.\n",
    "conversation.predict(input=\"What does Microsoft do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What does Microsoft do?\n",
      "AI: Microsoft is a multinational technology company that develops, manufactures, licenses, supports, and sells computer software, consumer electronics, personal computers, and related services. They are best known for their operating system, Microsoft Windows, which is used by the majority of personal computers worldwide. Microsoft also offers a wide range of other software products, including the Microsoft Office suite, which includes popular applications like Word, Excel, and PowerPoint. Additionally, they provide cloud services through their Azure platform, develop video game consoles like the Xbox, and have a presence in the hardware market with products like the Surface line of tablets and laptops.\n",
      "Human: How can they partner together with NASA?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Microsoft has a long-standing partnership with NASA. They collaborate on various projects and initiatives that involve the use of technology in space exploration and research. One notable example is the use of Microsoft's HoloLens mixed reality headset in NASA's Project Sidekick, which aims to provide astronauts with virtual assistance during space missions. Microsoft also works with NASA on data analysis and visualization tools, as well as cloud computing solutions to support the agency's scientific research and data processing needs. Additionally, Microsoft has partnered with NASA to develop educational programs and initiatives to inspire and engage students in STEM fields.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding more user input to the conversation chain keeps the memory, see the verbose output below.\n",
    "conversation.predict(input=\"How can they partner together with NASA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Tools\n",
    "\n",
    "You can set custom tools to be used, for example here is the bing search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About. I help companies transform into data driven organisations so they can become predictive instead of reactive. I’ve created a ‘data adoption framework’ that brings organisation from strategy...'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"BING_SUBSCRIPTION_KEY\"] = \"\"\n",
    "os.environ[\"BING_SEARCH_URL\"] = \"https://api.bing.microsoft.com/v7.0/search\"\n",
    "\n",
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "search = BingSearchAPIWrapper(k=1)\n",
    "search.run(\"Dave Enright from Singapore on LinkedIn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use agents to orchestrate multiple tools together\n",
    "\n",
    "Below the agent is being told to use the Bing Search tool when it needs to answer current events.The agent will use this tool based on the kinds of questions the user is asking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\n",
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "\n",
    "# The tool description helps langchain work out when to use the tool. Be careful with your descriptions as its not completely deterministic.\n",
    "search = BingSearchAPIWrapper(k=1)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"Use this when you need up to date information that is timely\",\n",
    "        return_direct=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the agent to use the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Yes.\n",
      "Follow up: What type of places are you looking for?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mYou can search for areas of interest, things to do, or notable locations in Google Maps. Find places like nearby museums, new restaurants, and popular bars and clubs. You can also find ratings and...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You can search for areas of interest, things to do, or notable locations in Google Maps. Find places like nearby museums, new restaurants, and popular bars and clubs. You can also find ratings and...'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple, Any, Union\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "# There are different kinds of agents documented on the langchain website. This particular one is basically just a chat with search agent.\n",
    "self_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\n",
    "self_ask_with_search.run(\"Give me a list of to places to visit where weather is not too hot and do not have a lot of tourists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who Leo DiCaprio's girlfriend is and then calculate her age raised to the 0.43 power.\n",
      "Action: Bing Search\n",
      "Action Input: \"Leo DiCaprio girlfriend\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m<b>Leonardo DiCaprio</b> &amp; Camila Morrone&#39;s Full Relationship Timeline Celebrity Celebrity News Everything You Need to Know About <b>Leonardo DiCaprio</b> and Camila Morrone&#39;s Relationship From the beginning... <b>Leonardo</b> <b>DiCaprio</b> started dating hot 25 year olds back in the roaring &#39;90s and...truly nothing has changed! Other than the people he&#39;s dating, of course, because lord knows this man won&#39;t stay... <b>Leonardo</b> <b>DiCaprio</b> recently split with his <b>girlfriend</b> of several years, Camila Morrone. In the past, the &quot;Titanic&quot; actor has been romantically linked to Gisele Bündchen, Rihanna, and more. <b>DiCaprio</b>&#39;s tendency to date much younger women has spawned countless memes and jokes online. <b>Leonardo</b> <b>DiCaprio</b> and Gigi Hadid were recently spotted at a pre-Oscars party, sparking interest once again in their rumored romance. The Revenant actor and the model first made headlines when they ... The actor made headlines last year for breaking up with his 25-year-old <b>girlfriend</b> Camila Morrone, further fuelling the theory that he does not date any women over that age. But now, the Hollywood star reportedly appears to be reaching even younger, having been linked to a 19-year-old Israeli model, Eden Polani. <b>DiCaprio</b>, 48, and Gill, 28, previously partied at the Chiltern Firehouse back in February. The pair appeared separately at the Cannes Film Festival last week. <b>Leonardo</b> <b>Dicaprio</b> hid from photographers. Busy Philipps has called out <b>Leonardo</b> <b>DiCaprio</b> for his rumoured relationship with a model who she says looks as old as her teenage daughter. The 43-year-old actor took to her Instagram Story on Sunday to share a post by Just Jared, which included a picture of model Meghan Roche, 22, on vacation with <b>DiCaprio</b>. In the caption of her since-deleted ... LEONARDO DiCaprio&#39;s list of girlfriends is almost as long as his movie credits. The Wolf of Wall Street star has been romantically linked to Gisele Bündchen, Blake Lively, and Gigi Hadid. 9. “<b>Leonardo</b> <b>DiCaprio</b>’s <b>girlfriend</b> is so young her high school experience was interrupted by COVID-19,” slammed one user Monday. “There’s a dog now who’s older than <b>Leonardo</b> <b>DiCaprio</b>’s...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Camila Morrone is Leo DiCaprio's girlfriend and she is 25 years old.\n",
      "Action: Calculator\n",
      "Action Input: 25^0.43\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 3.991298452658078\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Camila Morrone is Leo DiCaprio's girlfriend and her current age raised to the 0.43 power is 3.991298452658078.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Camila Morrone is Leo DiCaprio's girlfriend and her current age raised to the 0.43 power is 3.991298452658078.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"bing-search\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# There are different document loaders available, this is the basic PDF loader.\n",
    "loader = PyPDFLoader(\"Benefit_Options.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Contoso Electronics  \\nPlan and Benefit Packages', metadata={'source': 'Benefit_Options.pdf', 'page': 0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting into pages is useful to give page number back as a source\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: offers a wider range of prescription drug coverage than Northwind Standard. Both plans offer coverage \n",
      "for vision and dental services, as well as medical services.  \n",
      "Next Steps  \n",
      "We hope that this information has been helpful in understanding the differe nces between Northwind \n",
      "Health Plus and North\n",
      "2: Welcome to Contoso  Electronics ! We are excited to offer our employees two comprehensive health \n",
      "insurance plans through Northwind Health.  \n",
      "Northwind Health Plus  \n",
      "Northwind Health Plus is a comprehensive plan that provides comprehensive coverage for medical, \n",
      "vision, and dental services. T his pl\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# This is creating an index based on embeddings, but as you can see by the output it's a bit ugly to work with.\n",
    "\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings(chunk_size=1))\n",
    "docs = faiss_index.similarity_search(\"What's the difference between plus and standard?\", k=2)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work yet with Azure.\n",
    "#from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "# Using the vector store from the loader means langchain can abstract away a lot of the under the hood stuff.\n",
    "# Note I am using a local python package for the vectorDB. You may get errors.\n",
    "#index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'The main difference between Northwind Health Plus and Northwind Standard is that Northwind Health Plus offers more comprehensive coverage. Northwind Health Plus provides coverage for emergency services, mental health and substance abuse, and out-of-network services, while Northwind Standard does not. Additionally, Northwind Health Plus offers a wider range of prescription drug coverage, including generic, brand-name, and specialty drugs, whereas Northwind Standard only covers generic and brand-name drugs. Both plans offer coverage for vision and dental services, as well as medical services. \\nSOURCES: Benefit_Options.pdf'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "query = \"What's the difference between plus and standard?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain CSV example\n",
    "\n",
    "from langchain.agents import create_csv_agent\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "agent = create_csv_agent(llm, 'sample.csv', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the number of rows in the dataframe, I can use the `shape` attribute of the dataframe.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: df.shape[0]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m5\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe dataframe has 5 rows.\n",
      "Final Answer: 5\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"how many rows are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text_splitter = CharacterTextSplitter()\n",
    "\n",
    "with open(\"state_of_the_union.txt\", encoding=\"utf8\") as f:\n",
    "    sotu = f.read()\n",
    "texts = text_splitter.split_text(sotu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "docs = [Document(page_content=t) for t in texts[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'President Biden addresses the conflict between Russia and Ukraine in his State of the Union address, emphasizing the unity and resolve of the United States and its allies in holding Russia accountable. The US is taking action against Russian oligarchs and corrupt leaders, implementing economic sanctions, and providing support to Ukraine. American forces are not engaged in the conflict but are prepared to defend NATO allies. The US is implementing targeted sanctions on the Russian economy and releasing oil reserves to help with gas prices. The speaker highlights the rise of democracies and the support for Ukraine worldwide. The American Rescue Plan is praised for providing relief and support, contrasting with previous tax cuts. The importance of investing in infrastructure is emphasized, celebrating the passage of a bipartisan infrastructure law.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, the President did not say \"selamat pagi\" in English.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "query = \"Did the president say selamat pagi in english?\"\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test using cog search vector store.\n",
    "# reference: https://python.langchain.com/docs/integrations/vectorstores/azuresearch\n",
    "# !pip install --index-url=https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/ azure-search-documents==11.4.0a20230509004\n",
    "# !pip install azure-identity\n",
    "# !pip install azure-search-documents==11.4.0b6\n",
    "# !pip install langchain --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.vectorstores.azuresearch import AzureSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_address: str = 'https://globecogse.search.windows.net'\n",
    "vector_store_password: str = 'Jk3aMWZNUeOdjiPZxDWgaWS4lXOY001YQcRTZXVUbZAzSeCiFO0l'\n",
    "index_name: str = 'globevectorindex'\n",
    "\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(chunk_size=1)  \n",
    "vector_store: AzureSearch = AzureSearch(azure_search_endpoint=vector_store_address,  \n",
    "                                        azure_search_key=vector_store_password,  \n",
    "                                        index_name=index_name,  \n",
    "                                        embedding_function=embeddings.embed_query) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "loader = TextLoader('state_of_the_union.txt', encoding='utf-8')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1434, which is longer than the specified 1000\n",
      "Created a chunk of size 3063, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The president said that he has nominated Ketanji Brown Jackson to serve on the United States Supreme Court.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using retrievalQA\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(), verbose=False)\n",
    "qa.run('What did the president say about Ketanji Brown Jackson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And we all know—no matter what your ideology, we all know one of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. As I did 4 days ago, I've nominated a Circuit Court of Appeals—Ketanji Brown Jackson.\n"
     ]
    }
   ],
   "source": [
    "# using similarity search\n",
    "docus = vector_store.similarity_search(query=\"What did the president say about Ketanji Brown Jackson\", k=3, search_type='similarity')\n",
    "print(docus[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectoreStoreIndex (Not Working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use custom index with existing vectorstore. This opens up the ability to do query_with_sources etc\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "vector_store: AzureSearch = AzureSearch(azure_search_endpoint=vector_store_address,  \n",
    "                                        azure_search_key=vector_store_password,  \n",
    "                                        index_name=index_name,  \n",
    "                                        embedding_function=embeddings.embed_query) \n",
    "\n",
    "index_wrapper = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=AzureSearch,\n",
    "    embedding=OpenAIEmbeddings(chunk_size=1),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VectorstoreIndexCreator' object has no attribute 'query_with_sources'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat did the president say about Ketanji Brown Jackson\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m index_wrapper\u001b[39m.\u001b[39;49mquery_with_sources(query)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VectorstoreIndexCreator' object has no attribute 'query_with_sources'"
     ]
    }
   ],
   "source": [
    "# query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "# index_wrapper.query_with_sources(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Drive Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GoogleDriveLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = GoogleDriveLoader(document_ids=[\"1s_-umZEhfcNVcyEZ6FbOi0pPIr-UEadEbz_OyAerpqc\"])\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6042"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tracks offered by the cadetship program are AI/ML, Software Development, Product Management and Delivery, Security, IT Quality Management and Testing, and Platform Management.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document question and aswering\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "query = 'What are the tracks offered by the cadetship program?'\n",
    "chain = load_qa_chain(llm, chain_type='stuff', verbose=False)\n",
    "chain.run(input_documents=docs,question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text splitting \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs_split = text_splitter.split_documents(docs)\n",
    "\n",
    "len(docs_split)\n",
    "len(docs_split[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MWM5Zjc3MDAtZjVkZS00YjRiLWEwYTYtZTJkMGY2OTE5NDgy',\n",
       " 'Y2U0ZWNiOTEtNzBmZC00MzQzLWExYjEtYzQzYzlmYWUwOGVh',\n",
       " 'YTlhYTdkNTYtNzdiZC00MWIzLTkwMDUtNTYzYzdiMDc4ZTUy',\n",
       " 'YjE2ODY3ZWMtMDhmYi00ZGZiLThjMjMtYzMzNTg5YjVmOTRj',\n",
       " 'NTkxNzA4ZTYtMWU5OC00MDQ3LTg5NzMtZmRjNTU2M2QxYTAw',\n",
       " 'MGUxNzBmOGMtYTI4Ny00OGQ3LWEyNjktMWIzYjE0ZWMwNzdk',\n",
       " 'MWFmYTFlNDQtZjVlNS00ZWFhLWIxMzQtMzJlYTUwNDZiN2Jh']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "vector_store_address: str = 'https://globecogse.search.windows.net'\n",
    "vector_store_password: str = 'Jk3aMWZNUeOdjiPZxDWgaWS4lXOY001YQcRTZXVUbZAzSeCiFO0l'\n",
    "index_name: str = 'globevectorindex'\n",
    "\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(chunk_size=1)  \n",
    "vector_store: AzureSearch = AzureSearch(azure_search_endpoint=vector_store_address,  \n",
    "                                        azure_search_key=vector_store_password,  \n",
    "                                        index_name=index_name,  \n",
    "                                        embedding_function=embeddings.embed_query) \n",
    "                                        \n",
    "vector_store.add_documents(documents=docs_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tracks offered by the cadetship program are AI/ML, Software Development, Product Management and Delivery, Security, IT Quality Management and Testing, and Platform Management.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(), verbose=False)\n",
    "qa.run('What are the tracks offered by the cadetship program?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tracks offered by the cadetship program are AI/ML, Software Development, Product Management and Delivery, Security, IT Quality Management and Testing, and Platform Management.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever())\n",
    "qa.run('What are the tracks offered by the cadetship program?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
